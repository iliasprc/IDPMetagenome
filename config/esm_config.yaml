trainer:
  cwd: /home/ # working directory
  logger: IDP # logger name
  epochs: 50 # number of training epochs
  seed: 123 # randomness seed
  cuda: True # use nvidia gpu
  gpu: 0 # id of gpu
  save: True # save checkpoint
  batch_size: 1
  dim: 256
  load: True # load pretrained checkpoint
  gradient_accumulation: 256 # gradient accumulation steps
  pretrained_cpkt: /home/iliask/PycharmProjects/MScThesis/checkpoints/dataset_DM/model_esm1_t12_85M_UR50S/date_08_11_2021_22.08.09/_model_best_checkpoint.pth.tar
  log_interval: 1000 # print statistics every log_interval
  model:
    name: IDP_esm1_t6_43M_UR50S # model name  [mobilenet_v2,COVIDNet_small]
    pretrained: False
    optimizer: # optimizer configuration
      type: Adam # optimizer type
      lr: 1e-5 # learning rate
      weight_decay: 0.000001 # weight decay
    scheduler: # learning rate scheduler
      type: ReduceLRonPlateau # type of scheduler
      scheduler_factor: 0.8 # learning rate change ratio
      scheduler_patience: 3 # patience for some epochs
      scheduler_min_lr: 1e-5 # minimum learning rate value
      scheduler_verbose: 5e-6 # print if learning rate is changed
  dataloader:
    train:
      batch_size: 128 # batch size
      shuffle: True # shuffle samples after every epoch
      num_workers: 4 # number of thread for dataloader1
    val:
      batch_size: 512
      shuffle: False
      num_workers: 2
    test:
      batch_size: 128
      shuffle: False
      num_workers: 2
  dataset:
    input_data: data_dir
    name: MXD494 # dataset name
    type: bce # multi_target or clf
    use_elmo: True
    train:
      augmentation: True # do augmentation to video
    val:
      augmentation: False
    test:
      augmentation: False