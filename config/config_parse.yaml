cwd: /home/ # working directory
logger: IDP # logger name
epochs: 30 # number of training epochs
seed: 123 # randomness seed
cuda: True # use nvidia gpu
gpu: 0 # id of gpu
save: True # save checkpoint
batch_size: 1
dim: 128
layers: 2
heads: 2
load: False # load pretrained checkpoint
gradient_accumulation: 8 # gradient accumulation steps
pretrained_cpkt: None
log_interval: 1000 # print statistics every log_interval
model: idprnn # model name  [mobilenet_v2,COVIDNet_small]
pretrained: False
optimizer:  AdamW # optimizer type
lr: 1e-5 # learning rate
weight_decay: 0.00001 # weight decay
scheduler: ReduceLRonPlateau # type of scheduler
scheduler_factor: 0.8 # learning rate change ratio
scheduler_patience: 3 # patience for some epochs
scheduler_min_lr: 1e-5 # minimum learning rate value
scheduler_verbose: 5e-6 # print if learning rate is changed
dataset_type: classification
num_workers: 2
shuffle: True # shuffle samples after every epoch
dataset: MXD494
input_data: data_dir
name: MXD494 # dataset name
use_elmo: False
train_augmentation: True # do augmentation to video
val_augmentation: False
